{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/YongBeomKim/nltk_rnd/raw/master/data/test.jpg\">\n",
    "\n",
    "# **Project4 | word2vec**\n",
    "\n",
    "## **1 데이터 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! apt-get update\n",
    "# ! apt-get install g++ openjdk-8-jdk \n",
    "# ! pip3  install  nltk konlpy wordcloud matplotlib gensim \n",
    "\n",
    "# ! apt-get install fonts-nanum-coding\n",
    "# ! apt-get install fontconfig\n",
    "# ! fc-cache -fv\n",
    "# ! cp /usr/share/fonts/truetype/nanum/Nanum* /usr/local/lib/python3.6/dist-packages/matplotlib/mpl-data/fonts/ttf/\n",
    "# ! rm -rf /content/.cache/matplotlib/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_text = \"https://raw.githubusercontent.com/YongBeomKim/nltk_tutorial/master/data/movie_memories_of_murder_2003.txt\"\n",
    "font_file   = \"/usr/local/lib/python3.6/dist-packages/matplotlib/mpl-data/fonts/ttf/NanumGothicCoding.ttf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "movies = pd.read_csv('data/movies_metadata.csv', usecols=['original_title', 'overview', 'title'], low_memory=False)\n",
    "movies = movies.dropna(axis=0)\n",
    "print(movies.shape)\n",
    "\n",
    "movie_plot_li = movies['overview']\n",
    "movie_info_li = movies['title']\n",
    "movies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import matplotlib as mpl        \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "font_name   = fm.FontProperties(fname=font_file, size=10).get_name()\n",
    "plt.rc('font', family=font_name)\n",
    "fm._rebuild()\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트를 줄단위로 끊어서 불러온뒤\n",
    "# Token 단위로, 한글명사들을 추출한다\n",
    "def txtnoun(sentences , skip=False, tags=['Noun'], stem=True, set_tokens=False):\n",
    "    import re\n",
    "    from konlpy.tag import Okt\n",
    "    twitter = Okt()\n",
    "    result  = []\n",
    "    sentences = sentences.replace('\\n', '\\n|')\n",
    "    sentences = sentences.split('|')\n",
    "    for content in sentences:\n",
    "        texts      = content.replace('\\n', '') # 해당줄의 줄바꿈 내용 제거\n",
    "        tokenizer  = re.compile(r'[^ ㄱ-힣]+')  # 한글과 띄어쓰기를 제외한 모든 글자를 선택\n",
    "        token_data = tokenizer.sub('', texts)  # 한글과 띄어쓰기를 제외한 모든 부분을 제거\n",
    "        token_data = token_data.split(' ')\n",
    "        sentence   = []\n",
    "\n",
    "        for token in token_data:\n",
    "            # skip 대상이 없을 떄\n",
    "            if skip == False:\n",
    "                chk_tok = twitter.pos(token, stem=stem)\n",
    "                chk_tok = [temp[0]  for temp in chk_tok   if temp[1] in tags]\n",
    "                ckeck   = \"\".join(chk_tok)\n",
    "                if len(ckeck) > 1:\n",
    "                    sentence.append(ckeck)\n",
    "\n",
    "            # skip 내용이 있을 때\n",
    "            else:\n",
    "                if token.strip() in skip.keys():\n",
    "                    result.append(skip[token.strip()])\n",
    "                else:\n",
    "                    chk_tok = twitter.pos(token, stem=stem)\n",
    "                    chk_tok = [temp[0] for temp in chk_tok if temp[1] in tags]\n",
    "                    ckeck   = \"\".join(chk_tok)\n",
    "\n",
    "                    # 전처리가 끝난 결과가 skip에 해당여부 판단\n",
    "                    if ckeck.strip() in skip.keys():\n",
    "                        result.append(skip[ckeck.strip()])\n",
    "                    elif len(ckeck) > 1:\n",
    "                        sentence.append(ckeck)\n",
    "\n",
    "        # 단락별 작업이 끝난 뒤 '\\n'를 덧붙여서 작업을 종료\n",
    "        temp = \"\".join(sentence)\n",
    "        if len(temp) > 1:\n",
    "            sentence = \" \".join(sentence)\n",
    "            sentence += \"\\n\"\n",
    "            result.append(sentence)\n",
    "\n",
    "    if set_tokens == True:\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        set_token = word_tokenize(\" \".join(result))\n",
    "        return list(set(set_token))\n",
    "\n",
    "    else:\n",
    "        return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skips = {'두만':'박두만', '태윤':'서태윤', '용구':'조용구', '귀옥':'권귀옥', \n",
    "         '희봉':'구희봉', '동철':'신동철', '광호':'백광호', '병순':'조병순', \n",
    "         '해일':'박해일', '광호의':'백광호', '백광호의':'백광호'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
